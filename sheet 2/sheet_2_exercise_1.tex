\section{}





\subsection{}

\begin{lemma}
  Es sei $R$ ein Ring.
  \begin{enumerate}
    \item
      Für alle $x \in R$ gilt $0 \cdot x = 0 = x \cdot 0$.
    \item
      Für alle $x, y \in R$ gilt $(-x)y = -(xy) = x(-y)$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item
      Es gilt
      \[
          0 \cdot x
        = (0 + 0) \cdot x
        = 0 \cdot x + 0 \cdot x,
      \]
      und durch Subtraktion von $0 \cdot x$ ergibt sich, dass $0 = 0 \cdot x$.
      Analog ergibt sich, dass $x \cdot 0 = 0$.
    \item
      Es gilt
      \[
          xy + (-x)y
        = (x + (-x))y
        = 0 \cdot y
        = 0,
      \]
      weshalb $(-x)y = -(xy)$.
      Analog ergibt sich, dass $x(-y) = -(xy)$.
    \qedhere
  \end{enumerate}
\end{proof}

Es gilt $0 \in \ringcenter{R}$ da $0 \cdot y = 0 = y \cdot 0$ für alle $y \in R$.
Für $x_1, x_2 \in \ringcenter{R}$ gilt
\[
    (x_1 + x_2) y
  = x_1 y + x_2 y
  = y x_1 + y x_2
  = y (x_1 + x_2) 
  \qquad
  \text{für alle $y \in R$},
\]
und somit auch $x_1 + x_2 \in \ringcenter{R}$.
Für jedes $x \in R$ gilt
\[
    (-x) y
  = -(xy)
  = -(yx)
  = y (-x)
  \qquad
  \text{für alle $y \in R$},
\]
und somit auch $-x \in \ringcenter{R}$.
Insgesamt zeigt dies, dass $\ringcenter{R}$ eine Untergruppe der additiven Gruppe von $R$ ist.

Es gilt $1 \in \ringcenter{R}$ da $1 \cdot y = y = y \cdot 1$ für alle $y \in R$.
Für alle $x_1, x_2 \in \ringcenter{R}$ gilt
\[
    (x_1 x_2) y
  = x_1 x_2 y
  = x_1 y x_2
  = y x_1 x_2
  = y (x_1 x_2)
  \qquad
  \text{für alle $y \in R$},
\]
und somit auch $x_1, x_2 \in \ringcenter{R}$.
Insgesamt zeigt dies, dass $\ringcenter{R}$ ein Unterring von $R$ ist.

Zusätzlich bemerken wir noch, dass für jedes $x \in \ringcenter{R}$ mit $x \in \unitgroup{R}$ auch $x^{-1} \in \ringcenter{R}$ gilt, denn
\[
    x^{-1} y
  = x^{-1} y x x^{-1}
  = x^{-1} x y x^{-1}
  = y x^{-1}
  \qquad
  \text{für alle $y \in R$}.
\]





\subsection{}
Es sei $K$ ein Körper.
Wir zeigen, dass
\[
    \ringcenter{\matrices{n}{K}}
  = K \cdot I
  = \{\lambda \cdot I \suchthat \lambda \in K\}
\]
gilt, wobei $I \in \matrices{n}{K}$ die Einheitsmatrix bezeichnet.
Dass $K \cdot I \subseteq \ringcenter{\matrices{n}{K}}$ ergibt sich direkt daraus, dass
\[
    (\lambda I) A
  = \lambda A
  = A \cdot (\lambda I)
  \qquad
  \text{für alle $\lambda \in K$, $A \in \matrices{n}{K}$}.
\]
Andererseits sei $C \in \ringcenter{\matrices{n}{K}}$.
Für alle $i,j = 1, \dotsc, n$ sei $E_{ij} \in \matrices{n}{\real}$ die Matrix deren $(i,j)$-ter Eintrag $1$ ist, und deren andere Einträge alle $0$ sind, d.h.\ es gilt
\[
    (E_{ij})_{kl}
  = \begin{cases}
      1 & \text{falls $(k,l) = (i,j)$}, \\
      0 & \text{sonst},
    \end{cases}
    \qquad
    \text{für alle $k,l = 1, \dotsc, n$}.
\]
Wir zeigen nun in zwei Schritten, dass $C = \lambda \cdot I$ für ein $\lambda \in K$ gilt:
In einem ersten Schritt zeigen wir, dass $C$ eine Diagonalmatrix ist, und in dem darauffolgenden zweiten Schritt zeigen wir, dass alle Diagonaleinträge von $C$ gleich sind.

Wir geben die Rechnungen zunächst einer kompakte Form an.
Anschließend geben wir die Argumentation noch einmal in einer längeren, dafür aber anschaulicheren Form an.



\subsubsection{Kompakte Version}

\begin{itemize}
  \item
    Wir zeigen, dass $C$ eine Diagonalmatrix ist:
    Für jede Matrix $A \in \matrices{n}{K}$ gilt $CA = AC$.
    In Koeffizienten bedeutet dies, dass
    \begin{equation}
      \label{equation: equality in coefficients}
        \sum_{k=0}^n C_{ik} A_{kj}
      = (CA)_{ij}
      = (AC)_{ij}
      = \sum_{k=0}^n A_{ik} C_{kj}
    \end{equation}
    für alle $A \in \matrices{n}{K}$ und $i,j = 1, \dotsc n$ gilt.
    Indem wir die Matrix $A  = E_{ii}$ betrachten, erhalten wir dabei zum einen, dass
    \[
        \sum_{k=0}^n C_{ik} (E_{ii})_{kj}
      = \sum_{k=0}^n C_{ik} \delta_{ik} \delta_{ij}
      = \delta_{ij} C_{ii}
      \qquad
      \text{für alle $i, j = 1, \dotsc, n$},
    \]
    und zum anderen, dass
    \[
          \sum_{k=0}^n (E_{ii})_{ik} C_{kj}
        = \sum_{k=0}^n \delta_{ik} C_{kj}
        = C_{ij}
        \quad
        \text{für alle $i, j = 0, \dotsc, n$}.
    \]
    Für alle $1 \leq i \neq j \leq n$ erhalten wir somit aus \eqref{equation: equality in coefficients}, dass $0 = \delta_{ij} C_{ii} = C_{ij}$ gilt.
    (Für $i = j$ erhalten wir nur die triviale Aussage, dass $C_{ii} = \delta_{ij} C_{ii} = C_{ij} = C_{ii}$ gilt.)
    Das zeigt, dass $C$ eine Diagonalmatrix ist.
    
  \item
    Es seien nun $\lambda_1, \dotsc, \lambda_n \in K$ die Diagonaleinträge von $C$, d.h.\ es gelte $C_{ii} = \lambda_i$ für alle $i = 1, \dotsc, n$.
    Dann gilt $C_{ik} = \delta_{ik} \lambda_i$ für alle $i, k = 1, \dotsc, n$, bzw.\ äquivalent $C_{kj} = \delta_{jk} \lambda_j$ für alle $j, k = 1, \dotsc, n$.
    Die beiden Seiten von Gleichung \eqref{equation: equality in coefficients} vereinfacht sich somit zu
    \[
        \sum_{k=0}^n C_{ik} A_{kj}
      = \sum_{k=0}^n \delta_{ik} \lambda_i A_{kj}
      = \lambda_i A_{ij}
      \quad\text{und}\quad
        \sum_{k=0}^n A_{ik} C_{kj}
      = \sum_{k=0}^n A_{ik} \delta_{jk} \lambda_j
      = \lambda_j A_{ij},
    \]
    und Gleichung \eqref{equation: equality in coefficients} selbst vereinfacht sich somit zu
    \[
        \lambda_i A_{ij}
      = \lambda_j A_{ij}
      \qquad
      \text{für alle $A \in \matrices{n}{K}$ und $i, j = 1, \dotsc, n$}.
    \]
    Indem wir die Matrix $A = E_{ij}$ mit $A_{ij} = 1$ betrachten, erhalten wir somit, dass $\lambda_i = \lambda_j$ für alle $i,j = 1, \dotsc, n$ gilt.
    Also gilt $\lambda_1 = \dotsb = \lambda_j \eqqcolon \lambda$ und somit $C = \lambda I$.
\end{itemize}



\subsubsection{Anschauliche Version}

Wir wollen zunächst eine Anschauung dafür entwickeln, wie Multiplikation mit Diagonalmatrizen funktioniert:

\begin{observation}
  \label{observation: multiplication with diagonal matrices}
  Sind $D_1 \in \matrices{m}{K}$ und $D_2 \in \matrices{n}{K}$ zwei Diagonalmatrizen
  \[
      D_1
    = \begin{pmatrix}
        \lambda_1 &         &           \\
                  & \ddots  &           \\
                  &         & \lambda_m 
      \end{pmatrix}
    \quad\text{und}\quad
      D_2
    = \begin{pmatrix}
        \mu_1 &         &       \\
              & \ddots  &       \\
              &         & \mu_n 
      \end{pmatrix},
  \]
  so lassen sich für eine beliebige Matrix $A \in \mnatrices{m}{n}{K}$ die Produkte $D_1 A$ und $A D_2$ als
  \[
      D_1 A
    = \begin{pmatrix}
        \lambda_1 A_{11}  & \cdots  & \lambda_1 A_{1n}  \\
        \vdots            & \ddots  & \vdots            \\
        \lambda_m A_{m1}  & \cdots  & \lambda_m A_{mn}
      \end{pmatrix}
    \quad\text{und}\quad
      A D_2
    = \begin{pmatrix}
        \mu_1 A_{11}  & \cdots  & \mu_n A_{1n}  \\
        \vdots        & \ddots  & \vdots        \\
        \mu_1 A_{m1}  & \cdots  & \mu_n A_{mn}
      \end{pmatrix}
  \]
  berechnen.
  Durch Multiplikation mit $D_1$ von links wird also die $i$-te Zeile von $A$ mit $\lambda_i$ multipliziert, und durch Multiplikation mit $D_2$ von rechts wird die $j$-te Spalte von $A$ mit $\mu_j$ multipliziert.
  Dies lässt sich schematisch als
  \begin{align*}
        \begin{pmatrix}
          \lambda_1 &         &           \\
                    & \ddots  &           \\
                    &         & \lambda_m
        \end{pmatrix}
        \cdot
        \renewcommand\arraystretch{0.7}
        \begin{pmatrix}  
          & & z_1     & & \\
        \cmidrule(lr){1-5}
          & & \vdots  & & \\
        \cmidrule(lr){1-5}
          & & z_m     & & 
        \end{pmatrix}
    &=  \renewcommand\arraystretch{0.6}
        \begin{pmatrix}  
          & & \lambda_1 z_1 & & \\
        \cmidrule(lr){1-5}
          & & \vdots        & & \\
        \cmidrule(lr){1-5}
          & & \lambda_m z_m & &
        \end{pmatrix},
  \shortintertext{und}
        \left(
        \begin{array}{@{} c|c|c @{}}
              &         &     \\
          s_1 & \cdots  & s_n \\
              &         &
        \end{array}
        \right)
        \cdot
        \begin{pmatrix}
          \mu_1 &         &       \\
                & \ddots  &       \\
                &         & \mu_n
        \end{pmatrix}
    &=  \left(
        \begin{array}{@{} c|c|c @{}}
                    &         &           \\
          \mu_1 s_1 & \cdots  & \mu_n s_n \\
                    &         &           \\
        \end{array}
        \right)
  \end{align*}
  darstellen.
\end{observation}

Wir zeigen nun in den angekündigten zwei Schritten, dass $C = \lambda \cdot I$ für ein $\lambda \in K$:
\begin{itemize}
  \item
    Wir zeigen zunächst, dass $C$ eine Diagonalmatrix ist:
    Hierfür sei $1 \leq i \leq n$.
    Dann ist $E_{ii}$ eine Diagonalmatrix, deren $i$-tere Diagonaleintrag $1$ ist, und deren Diagonaleinträge sonst alle verschwinden.
    Da $C \in \ringcenter{\matrices{n}{K}}$ gilt, erhalten wir, dass $C E_{ii} = E_{ii} C$.
    Nach Beobachtunng~\ref{observation: multiplication with diagonal matrices} entsteht dabei die Matrix $C E_{ii}$ aus $C$, indem die $i$-te Spalte unverändert bleibt, aber alle anderen Spalten durch die Nullspalte ersetzt werden.
    Analog entsteht $E_{ii} C$ aus $C$, indem die $i$-te Zeilen unverändert bleibt, aber alle anderen Zeilen durch die Nullzeile ersetzt werden.
    Anschaulich gesehen gilt also, dass
    \[
      C E_{ii}
      =
      \begin{pmatrix}
        0       & \cdots  & 0       & C_{1i}    & 0       & \cdots  & 0       \\
        \vdots  & \ddots  & \vdots  & \vdots    & \vdots  & \ddots  & \vdots  \\
        0       & \cdots  & 0       & C_{ii}    & 0       & \cdots  & 0       \\
        \vdots  & \ddots  & \vdots  & \vdots    & \vdots  & \ddots  & \vdots  \\
        0       & \cdots  & 0       & C_{ni}    & 0       & \cdots  & 0
      \end{pmatrix}
      \quad\text{und}\quad
      E_{ii} C
      = \begin{pmatrix}
        0       & \cdots  & 0       & \cdots  & 0       \\
        \vdots  & \ddots  & \vdots  & \ddots  & \vdots  \\
        0       & \cdots  & 0       & \cdots  & 0       \\
        C_{i1}  & \cdots  & C_{ii}  & \cdots  & C_{in}  \\
        0       & \cdots  & 0       & \cdots  & 0       \\
        \vdots  & \ddots  & \vdots  & \ddots  & \vdots  \\
        0       & \cdots  & 0       & \cdots  & 0
      \end{pmatrix}.
    \]
    Da nach Annahme $C E_{ii} = E_{ii} C$ gilt, erhalte wir, dass in der $i$-ten Zeile und $i$-ten Spalte von $C$ bis auf den gemeinsamen Eintrag $C_{ii}$ alle anderen Einträge verschwinden müssen, d.h.\ für alle $j = 0, \dotsc, \hat{i}, \dotsc, n$ gilt $C_{ij} = 0$ und $C_{ji} = 0$.
    Da dies für alle $i = 1, \dotsc, n$ gilt, erhalten wir, dass in jeder Spalte (und in jeder Zeile) von $C$ alle nicht-Diagonaleinträge verschwinden.
    Also ist $C$ eine Diagonalmatrix.
    
  \item
    Für alle $i = 1, \dotsc, n$ sei $\lambda_i \in K$ der $i$-te Diagonaleintrag von $C$, d.h.\ es gelte
    \[
        C
      = \begin{pmatrix}
          \lambda_1 &         &           \\
                    & \ddots  &           \\
                    &         & \lambda_n
        \end{pmatrix}.
    \]
    Wir zeigen, dass alle Diagonaleinträge von $C$ bereits gleich sind:
    Es seien $1 \leq i \neq j \leq n$.
    Der einzige nicht-verschwindende Eintrag von $E_{ij}$ befindet sich in der $i$-ten Zeile und $j$-ten Spalte von $E_{ij}$.
    Aus Beobachtung~\ref{observation: multiplication with diagonal matrices} folgt nun, dass $C E_{ij} = \lambda_i E_{ij}$ und $E_{ij} C = \lambda_j E_{ij}$.
    Anschaulich lässt sich die Anwendung von Beobachtung~\ref{observation: multiplication with diagonal matrices} als
    \begin{gather*}
        C E_{ij}
      = \begin{pmatrix}
          \lambda_1 &         &           &         &         &           \\
                    & \ddots  &           &         &         &           \\
                    &         & \lambda_i &         &         &           \\
                    &         &           & \ddots  &         &           \\
                    &         &           &         & \ddots  &           \\
                    &         &           &         &         & \lambda_n
        \end{pmatrix}
        \begin{pmatrix}
          0 &         &         &           &         &   \\
            & \ddots  &         &           &         &   \\
            &         & \ddots  & 1         &         &   \\
            &         &         & \ddots    &         &   \\
            &         &         &           & \ddots  &   \\
            &         &         &           &         & 0
        \end{pmatrix}
      = \begin{pmatrix}
          0 &         &         &           &         &   \\
            & \ddots  &         &           &         &   \\
            &         & \ddots  & \lambda_i &         &   \\
            &         &         & \ddots    &         &   \\
            &         &         &           & \ddots  &   \\
            &         &         &           &         & 0
        \end{pmatrix}
    \shortintertext{und}
        E_{ij} C
      = \begin{pmatrix}
          0 &         &         &           &         &   \\
            & \ddots  &         &           &         &   \\
            &         & \ddots  & 1         &         &   \\
            &         &         & \ddots    &         &   \\
            &         &         &           & \ddots  &   \\
            &         &         &           &         & 0
        \end{pmatrix}
        \begin{pmatrix}
          \lambda_1 &         &         &           &         &           \\
                    & \ddots  &         &           &         &           \\
                    &         & \ddots  &           &         &           \\
                    &         &         & \lambda_j &         &           \\
                    &         &         &           & \ddots  &           \\
                    &         &         &           &         & \lambda_n
        \end{pmatrix}
      = \begin{pmatrix}
          0 &         &         &           &         &   \\
            & \ddots  &         &           &         &   \\
            &         & \ddots  & \lambda_j &         &   \\
            &         &         & \ddots    &         &   \\
            &         &         &           & \ddots  &   \\
            &         &         &           &         & 0
        \end{pmatrix}
    \end{gather*}
    notieren.
    Durch Vergleich der Einträge $(C E_{ij}) = \lambda_i$ und $(E_{ij} C) = \lambda_j$ ergibt sich hieraus, dass $\lambda_i = \lambda_j$ gilt.
    Das dies für alle $1 \leq i \neq j \leq n$ gilt, muss bereits $\lambda_1 = \dotsb = \lambda_n \eqqcolon \lambda$, und somit $C = \lambda I$.
\end{itemize}



\subsubsection{Weitere Bemerkungen}

\begin{remark}
  Die Idee der obigen Rechnung lässt sich grob wie folgt beschreiben:
  
  Es sei $C \in \matrices{n}{K}$.
  Dass $C$ mit einer Matrix $A \in \matrices{n}{K}$ kommutiert, dass also $CA = AC$ gilt, ist eine zusätzliche Bedingung an $C$.
  Dabei hängt es von der Matrix $A$ ab, wie restriktiv diese Bedingung ist:
  \begin{itemize}
    \item
      Betrachtet man etwa $A = I$, so erhält man die Bedingung $C = C$.
      Diese Bedingung liefert uns keine näheren Informationen über $C$.
    \item
      Betrachtet man $A = E_{ii}$ für ein $1 \leq i \leq n$, so liefert uns die Bedingungen $CA = AC$, dass in der $i$-ten Zeile und $i$-ten Spalte von $A$ bis auf den gemeinsamen Diagonaleintrag $A_{ii}$ jeder andere Diagonaleintrag verschwindet.
    \item
      Betrachtet man eine Diagonalmatrix $A$ mit paarweise verschiedenen Diagonaleinträgen $\lambda_1, \dotsc, \lambda_n \in K$, also
      \[
          A
        = \begin{pmatrix}
            \lambda_1 &         &           \\
                      & \ddots  &           \\
                      &         & \lambda_n
          \end{pmatrix}
          \quad\text{mit}\quad
          \text{$\lambda_i \neq \lambda_j$ für $i \neq j$},
      \]
      so erhalten wir durch Beobachtung~\ref{observation: multiplication with diagonal matrices} aus der Bedingung $CA = AC$, dass
      \[
          \lambda_j C_{ij}
        = (CA)_{ij}
        = (AC)_{ij}
        = \lambda_i C_{ij}
        \qquad
        \text{für alle $i, j = 1, \dotsc, n$}.
      \]
      Da $\lambda_i \neq \lambda_j$ für alle $1 \leq i \neq j \leq n$ gilt, folgt daraus, dass $C_{ij} = 0$ für alle $1 \leq i,j \leq n$ gilt.
      Also muss $C$ bereits eine Diagonalmatrix sein.
  \end{itemize}
  Je mehr Matrizen mit $C$ kommutieren, desto mehr Bedingungen werden an $C$ gestellt, und desto mehr Restriktionen gibt es bezüglich des Aussehens von $C$.
  
  Ist nun $C \in \ringcenter{ \matrices{n}{K} }$ so gilt für jede Matrix $A \in \matrices{n}{K}$ die entsprechende Bedingung $A C = C A$.
  Indem wir für $A$ „ausreichend viele“ passende Matrizen betrachten, erhalten wir unterschiedliche Restriktionen an die Form von $C$.
  Die Hoffnung ist, hierdurch bereits die gewünschte Form $C = \lambda \cdot I$ mit $\lambda \in K$ zu erzwingen.
  
  Wegen der Endlichdimensionalität von $\matrices{n}{K}$ lassen sich dabei die Famlie von $n^2 \cdot |K|$ vielen Bedingungen
  \[
    CA = AC
    \qquad
    \text{für alle $A \in \matrices{n}{K}$}
  \]
  auf bereits $n^2$ viele Bedingungen reduzieren:
  Ist $\{ A_1, \dotsc, A_{n^2} \}$ eine $K$-Basis von $\matrices{n}{K}$ und gilt $C A_i = A_i C$ für alle $i = 1, \dotsc, n^2$, so gilt bereits $C A = A C$ für alle $A \in \matrices{n}{K}$, denn mit $A = \sum_{i=1}^{n^2} \lambda_i A_i$ erhält man die Gleichungskette
  \[
      C A
    = C \left( \sum_{i=1}^{n^2} \lambda_i A_i \right)
    = \sum_{i=1}^{n^2} \lambda_i C A_i
    = \sum_{i=1}^{n^2} \lambda_i A_i C
    = \left( \sum_{i=1}^{n^2} \lambda_i A_i \right) C
    = A C.
  \]
  
  Betrachtet man nun die (naheliegende) Basis $\basis{B} = \{ E_{ij} \suchthat i, j = 1, \dotsc n \}$ von $\matrices{n}{K}$, so gilt für $C \in \matrices{n}{K}$ also, dass genau dann $C \in \ringcenter{ \matrices{n}{K} }$ gilt, wenn $C E_{ij} = E_{ij} C$ für alle $i, j = 1, \dotsc, n$ gilt.
  Da die Matrizen aus $\basis{B}$ eine möglichst einfache Form haben, sind die Bedingungen $C E_{ij} = E_{ij} C$ dabei ebenfalls möglich einfach.
\end{remark}

\begin{remark}
  Wir haben in unseren Beweis nur genutzt, dass $K$ ein kommutativer Ring ist.
  Mit unveränderten Beweis ergibt sich deshalb, dass
  \begin{alignat}{2}
    \label{equation: center of matrix ring for commutative ground rings}
      \ringcenter{ \matrices{n}{R} }
    &= R \cdot I
    &
    & \text{für jeden kommutativen Ring $R$}
  \intertext{
  gilt.
  Möchte man auch nicht-kommutative Ringe betrachten, so lässt sich \eqref{equation: center of matrix ring for commutative ground rings} zu
  }
    \label{equation: center of matrix ring for arbitrary ground rings}
      \ringcenter{ \matrices{n}{R} }
    &= \ringcenter{R} \cdot I
    &\qquad
    &\text{für jeden Ring $R$}
  \end{alignat}
  erweitern;
  ist dabei $R$ kommutativ, so gilt $\ringcenter{R} = R$ (dies ist nur eine Umformulierung der Kommutativität von $R$), und \eqref{equation: center of matrix ring for arbitrary ground rings} vereinfacht sich wieder zu \eqref{equation: center of matrix ring for commutative ground rings}.
  
  Ein Beweis für \eqref{equation: center of matrix ring for arbitrary ground rings} ergibt sich durch leichte Abänderung, bzw.\ Erweiterung unseres Beweises für \eqref{equation: center of matrix ring for commutative ground rings}:
  Es ergibt sich wie zuvor, dass $\ringcenter{R} \cdot I \subseteq \ringcenter{ \matrices{n}{R} }$ gilt.
  
  Zum Beweis der Inklusion $\ringcenter{ \matrices{n}{R} } \subseteq \ringcenter{R} \cdot I$ zeigt man für $C \in \ringcenter{ \matrices{n}{R} }$ ebenfalls zunächst, dass
  \begin{equation}
    \label{equation: central matrices are scalar matrices}
    C = r \cdot I
    \qquad
    \text{für ein $r \in R$}
  \end{equation}
  gilt.
  Im Fall $R = K$ haben wir für unseren Beweis von \eqref{equation: central matrices are scalar matrices} die Kommutativität von $K$ nicht genutzt;
  für den allgemeinen Fall können wir deshalb unseren zweischritten Beweis unverändert übernehmen.
  
  In einem dritten Schritt muss nun noch gezeigt werden, dass bereits $r \in \ringcenter{R}$ gilt.
  Dies ergibt sich daraus, dass für alle $s \in R$ die Gleichheitskette
  \[
      (rs) \cdot I
    = (r \cdot I)(s \cdot I)
    = C (s \cdot I)
    = (s \cdot I) C
    = (s \cdot I) (r \cdot I)
    = (sr) \cdot I
  \]
  gilt, und somit durch Vergleich der Diagonaleinträge bereits $rs = sr$ für alle $s \in R$.
\end{remark}





\subsection{}

Wir zeigen, dass
\[
    \ringcenter{ \polynomialring{R}{t} }
  = \left\{
      \sum_{i=0}^\infty a_i \in \polynomialring{R}{t}
    \suchthatscale
      \text{$a_i \in \ringcenter{R}$ für alle $i$}
    \right\}
  = \polynomialring{ \ringcenter{R} }{t}
\]
gilt.
Die zweite Gleichheit gilt, da es sich hierbei um die Definition von $\polynomialring{ \ringcenter{R} }{t}$ handelt.

Ist $p = \sum_{i=0}^\infty a_i t^i \in \polynomialring{ \ringcenter{R} }{t}$, so gilt $a_i b = b a_i$ für alle $b \in R$ und $i \geq 0$.
Für jedes $q = \sum_{j=0}^\infty b_j t^j \in \polynomialring{R}{t}$ gilt deshalb
\[
     p q 
   = \left( \sum_{i=0}^\infty a_i t^i \right) \left( \sum_{j=0}^\infty b_j t^j \right)
   = \sum_{i,j=0}^\infty a_i b_j t^{i+j}
   = \sum_{j,i=0}^\infty b_j a_i t^{j+i}
   = \left( \sum_{j=0}^\infty b_j t^j \right) \left( \sum_{i=0}^\infty a_i t^i \right)
   = q p.
\]
Also gilt $p \in \ringcenter{ \polynomialring{R}{t} }$.

Andererseits sei $p \in \ringcenter{ \polynomialring{R}{t} }$.
Dann gilt $p q = q p$ für jedes $q \in \polynomialring{R}{t}$.
Für alle $b \in R$ gilt dann
\[
    \sum_{i=0}^\infty (a_i b) t^i
  = \left( \sum_{i=0}^\infty a_i t^i \right) \left( b t^0 \right)
  = \left( b t^0 \right) \left( \sum_{i=0}^\infty a_i t^i \right)
  = \sum_{i=0}^\infty (b a_i) t^i.
\]
Für jedes $i \geq 0$ gilt also $b a_i = a_i b$ für alle $b \in R$ (denn zwei Polynome sind genau dann gleich, wenn alle ihre Koeffizienten gleich sind).
Dies bedeutet gerade, dass $a_i \in \ringcenter{R}$ für alle $i \geq 0$ gilt.
